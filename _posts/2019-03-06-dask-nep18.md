---
layout: post
title: Dask and the __array_function__ protocol
tagline: Advances on NEP-18
author: Peter Andreas Entschev
tags: [Dask, Dask-GLM, CuPy, Sparse]
theme: twitter
---
{% include JB/setup %}


Summary
-------

Dask is versatile for analytics parallelism, but there is still one issue to
leverage it to a broader spectrum: allowing it to transparently work with
[NumPy](https://www.numpy.org/)-like libraries. NumPy recently addressed this
issue in
[NEP-18](https://www.numpy.org/neps/nep-0018-array-function-protocol.html)
with the introduction of the ``__array_function__`` protocol. In short, the
protocol allows a NumPy function call to dispatch the appropriate NumPy-like
library implementation, depending on the array type given as input, thus
allowing Dask to remain agnostic of such libraries, internally calling just the
NumPy function, which automatically handles dispatching of the appropriate
library implementation, for example,
[CuPy](https://cupy.chainer.org/) or [Sparse](https://sparse.pydata.org/).

To understand what's the end goal of this change, consider the following
example:

```python
import numpy as np
import dask.array as da

x = np.random.random((5000, 1000))

d = da.from_array(x, chunks=(1000, 1000))

u, s, v = np.linalg.svd(x)
```

Now consider we want to speedup the SVD computation of a Dask array and offload
that work to a CUDA-capable GPU, we ultimately want to simply replace the NumPy
array ``x`` by a CuPy array and let NumPy do its magic via
``__array_function__`` protocol and dispatch the appropriate CuPy linear algebra
operations under the hood:

```python
import numpy as np
import cupy
import dask.array as da

x = cupy.random.random((5000, 1000))

d = da.from_array(x, chunks=(1000, 1000))

u, s, v = np.linalg.svd(x)
```

We could do the same for a Sparse array, or any other NumPy-like array that
supports the ``__array_function__`` protocol and the computation that we are
trying to perform.

Fixed Issues
------------

During the month of February, 2019, substantial progress has been made towards
deeper support of the ``__array_function__`` protocol among the different
projects, this trend will continue in March. Below we see a list of issues that
have been fixed or are in the process of review:

*   ``__array_function__`` protocol dependencies fixed in
    [CuPy PR #2029](https://github.com/cupy/cupy/issues/2029);
*   Dask issues using CuPy backend with mean() and moment()
    [Dask Issue #4481](https://github.com/dask/dask/issues/4481), fixed in
    [Dask PR #4513](https://github.com/dask/dask/pull/4513) and
    [Dask PR #4519](https://github.com/dask/dask/pull/4519);
*   Replace in SciPy the aliased NumPy functions that may not be available in
    libraries like CuPy, fixed in
    [SciPy PR #9888](https://github.com/scipy/scipy/pull/9888);
*   Allow creation of arbitrary shaped arrays, using the input array as
    reference for the new array to be created, under review in
    [NumPy PR #13043](https://github.com/numpy/numpy/issues/13043);
*   Multithreading with CuPy first identified in
    [Dask Issue #4487](https://github.com/dask/dask/issues/4487),
    [CuPy Issue #2045](https://github.com/cupy/cupy/issues/2045) and
    [CuPy Issue #1109](https://github.com/cupy/cupy/issues/1109), now under
    review in [CuPy PR #2053](https://github.com/cupy/cupy/pull/2053);
*   Calling Dask's ``flatnonzero()`` on CuPy array missing ``cupy.compress()``,
    first identified in
    [Dask Issue #4497](https://github.com/dask/dask/issues/4497), under review
    in [Dask PR #4548](https://github.com/dask/dask/pull/4548).

Known Issues
------------

Currently, one of the biggest issues we are tackling relates to the
[Dask issue #4490](https://github.com/dask/dask/issues/4490) we first identified
when calling Dask's ``diag()`` on a CuPy array. This requires some change on the
Dask ``Array`` class, and subsequent changes throughout large parts of the Dask
codebase. I will not go into too much detail here, but the way we are handling
this issue consists in adding a new attribute ``_meta`` to Dask ``Array`` in
substitution to the simple ``dtype`` that currently exists. This new attribute
will not only hold the ``dtype`` information, but also an empty array of the
backend type used to create the ``Array`` in the first place, thus allowing us
to internally reconstruct arrays of the backend type, without having to know
explicitly whether it's a NumPy, CuPy, Sparse or any other NumPy-like
array. For additional details, please refer to
[Dask Issue #2977](https://github.com/dask/dask/issues/2977).

We have identified some more issues with ongoing discussions:

*   Using Sparse as a Dask backend, discussed in
    [Dask Issue #4523](https://github.com/dask/dask/issues/4523);
*   Calling Dask's ``fix()`` on CuPy array depends on ``__array_wrap__``,
    discussed in [Dask Issue #4496](https://github.com/dask/dask/issues/4496)
    and [CuPy Issue #589](https://github.com/cupy/cupy/issues/589);
*   Allow coercing of ``__array_function__``, discussed in
    [NumPy Issue #12974](https://github.com/numpy/numpy/issues/12974).


Dask-GLM
--------

By combining the fixes mentioned previously to support the use of CuPy as a
backend for Dask we can already see some results in Dask-GLM. We have been
working to support the aforementioned changes in NumPy, CuPy and Dask in
Dask-GLM under the [Dask-GLM PR #75](https://github.com/dask/dask-glm/pull/75).
We also did some linear regression benchmarking of Dask-GLM solvers with Dask
and CuPy to compare. The results below were obtained from a training dataset
with 1000 features of 100 dimensions, and 1000 test features.

```
Solver admm with Dask took 22444.307 ms to fit and 41.299 ms to predict
Solver admm with CuPy took 263.742 ms to fit and 1.146 ms to predict

Solver lbfgs with Dask took 1561.047 ms to fit and 41.063 ms to predict
Solver lbfgs with CuPy took 13.935 ms to fit and 0.260 ms to predict

Solver newton with Dask took 785.150 ms to fit and 42.822 ms to predict
Solver newton with CuPy took 22.480 ms to fit and 0.248 ms to predict

Solver proximal_grad with Dask took 1902.620 ms to fit and 45.349 ms to predict
Solver proximal_grad with CuPy took 11.700 ms to fit and 0.257 ms to predict

Solver gradient_descent with Dask took 2483.603 ms to fit and 42.605 ms to predict
Solver gradient_descent with CuPy took 14.580 ms to fit and 0.257 ms to predict
```

From the results above, we see Dask-GLM is capable of computing results 1-2
orders of magnitude faster using CuPy as a backend, compared to using Dask as a
backend on a single node. Note also that CuPy is running on a single GPU.


Future Work
-----------

There are several possibilities for a richer experience with Dask, some of which
could be very interesting in the short and mid-term are:

1.  Leverage Dask with
    [cuDF - GPU DataFrame Library](https://github.com/rapidsai/cudf);

2.  Profile CuPy's performance of matrix-matrix multiplication operations
    (GEMM), compare to matrix-vector multiplication operations (GEMV) for
    distributed Dask operation;

3.  Support for [more models in
    Dask-GLM](https://scikit-learn.org/stable/modules/linear_model.html);

4.  More comprehensive examples and benchmarks for Dask-GLM.
