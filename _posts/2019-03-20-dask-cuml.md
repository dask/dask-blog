---
layout: post
title: cuML and Dask hyperparameter optimization
tagline: hyperparameter optimization and dask
author: Benjamin Zaitlen
tags: [dask, GPU, RAPIDS,]
theme: twitter
---
{% include JB/setup %}

### Setup

- DGX-1 Workstation
- Host Memory: 512 GB
- GPU Tesla V100 x 8
- cudf 0.6
- cuml 0.6
- dask 1.1.4
- [Jupyter notebook](https://gist.github.com/quasiben/a96ce952b7eb54356f7f8390319473e4)

**TLDR; Hyper-parameter Optimization is functional but slow with cuML**

## cuML and Dask Hyper-parameter Optimization

cuML is an open source GPU accelerated machine learning library primarily
developed at NVIDIA and mirrors the [sklearn API](https://scikit-learn.org/).
The current suite of algorithms includes GLMs, Kalman Filtering, clustering,
and dimensionality reduction.  Many of these machine learning algorithms use
hyper-parameters.  These are parameters used during the model training process
but are not "learned" during the training.  Often these parameters are
coefficients or penalty thresholds and finding the "best" hyper parameter can be
computationally costly.  In the PyData community, we often reach to
[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
or
[RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV)
for easy definition of the search space for hyper-parameters -- this called hyper-parameter
optimization.  With in the Dask community, [Dask-ML](https://dask-ml.readthedocs.io/en/latest/) has incrementally improved the efficiency of hyper-parameter optimization by leveraging both sklearn and dask to use multi-core and
distributed schedulers: [Grid and RandomizedSearch with DaskML](https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html).


With the newly created drop-in replacement for sklearn, cuML, we experimented with Dask's GridSearchCV.  In the upcoming 0.6 release of cuML, the estimators are serializable and are functional within the sklearn/dask-ml framework, but slow compared with sklearn estimators.  And while speeds are slow now, we know how to boost performance, have filed several issues, and hope to show performance gains in future releases.

*All code and timing measurements can be found in this [Jupyter notebook](https://gist.github.com/quasiben/a96ce952b7eb54356f7f8390319473e4)*


## Fast Fitting!

cuML is fast!  But finding that speed requires developing a bit of GPU knowledge and some
intuition.  For example, there is a non-zero cost of moving data from device to GPU and, when data is "small" there are little to no performance gains.  "Small", currently might mean less than 100MB.

In the following example we use the [diabetes data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html)
set provided by slearn and linearly fit the data with [RidgeRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)


$$ \min\limits_w ||y - Xw||^2_2 + alpha * ||w||^2_2$$

[**alpha**](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html) is the hyper-parameter and we initially set to 1.

```python
import numpy as np
from cuml import Ridge as cumlRidge
import dask_ml.model_selection as dcv
from sklearn import datasets, linear_model
from sklearn.externals.joblib import parallel_backend
from sklearn.model_selection import train_test_split, GridSearchCV

X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2)

fit_intercept = True
normalize = False
alpha = np.array([1.0])

ridge = linear_model.Ridge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver='cholesky')
cu_ridge = cumlRidge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver="eig")

ridge.fit(X_train, y_train)
cu_ridge.fit(X_train, y_train)=
```

The above ran with timing measurements of:

- Sklearn Ridge: 28 ms
- cuML Ridge: 1.12 s

But the data is quite small, ~28KB.  Increasing the size to ~2.8GB and re-running we see significant gains:

```python
dup_ridge = linear_model.Ridge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver='cholesky')
dup_cu_ridge = cumlRidge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver="eig")

# move data from host to device
record_data = (('fea%d'%i, dup_data[:,i]) for i in range(dup_data.shape[1]))
gdf_data = cudf.DataFrame(record_data)
gdf_train = cudf.DataFrame(dict(train=dup_train))

#sklearn
dup_ridge.fit(dup_data, dup_train)

# cuml
dup_cu_ridge.fit(gdf_data, gdf_train.train)
```

With new timing measurements of:

- Sklearn Ridge: 4.82 s ± 694
- cuML Ridge: 450 ms ± 47.6

With more data we clearly see faster fitting times.  However, the time to move data to the GPU (through CUDF)
was 19.7s -- this is one area we can improve upon.

## Hyper-Parameter Optimization Experiments

So moving to the GPU can be costly, but once there, with larger data sizes, we gain significant performance
optimizations.  Naively, we thought, "well, we have GPU machine learning, we have distributed hyper-parameter optimization...
we *should* have GPU hyper-parameter optimization!"

Sklearn assumes a specific, but well defined API for its object.  Most estimators/classifiers in sklearn look like the following:

```python
class DummyEstimator(BaseEstimator):
    def __init__(self, params=...):
        ...

    def fit(self, X, y=None):
        ...

    def predict(self, X):
        ...

    def score(self, X, y=None):
        ...

    def get_params(self):
        ...

    def set_params(self, params...):
        ...
```

When we started experimenting with hyper-parameter optimization, we found a few API holes missing, these were
resolved, mostly handling matching argument structure and various getters/setters.

- [#271](https://github.com/rapidsai/cuml/pull/271)
- [#318](https://github.com/rapidsai/cuml/pull/318)
- [#330](https://github.com/rapidsai/cuml/pull/330)
- [#322](https://github.com/rapidsai/cuml/pull/322)

With holes plugged up we tested again.  Using the same diabetes data set, we are now performing hyper-parameter optimization
and searching over many alpha parameters for the best *scoring* alpha.

```python
params = {'alpha': np.logspace(-3, -1, 10)}
clf = linear_model.Ridge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver='cholesky')
cu_clf = cumlRidge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver="eig")

grid = GridSearchCV(clf, params, scoring='r2')
grid.fit(X_train, y_train)

cu_grid = GridSearchCV(cu_clf, params, scoring='r2')
cu_grid.fit(X_train, y_train)
```

Again, reminding ourselves that the data is small ~28KB, we don't expect to observe cuml performing faster than sklearn. Instead, we want to demonstrate functionality.  Fortunately, GridSearchCV from dask-ml adheres to the same API as sklearn; we can swap out GridsearchCV from sklearn and use dask-ml's which will use all of the GPUs we have available.

```python
params = {'alpha': np.logspace(-3, -1, 10)}
clf = linear_model.Ridge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver='cholesky')
cu_clf = cumlRidge(alpha=alpha, fit_intercept=fit_intercept, normalize=normalize, solver="eig")

grid = dcv.GridSearchCV(clf, params, scoring='r2')
grid.fit(X_train, y_train)

cu_grid = dcv.GridSearchCV(cu_clf, params, scoring='r2')
cu_grid.fit(X_train, y_train)
```

Timing Measurements:

| | sklearn | cuml |
| -----------| --- | -----|
| **GridSearchCV** | 88.4 ms ± 6.11 ms | 6.51 s ± 132 |
| **dask-ml GridSearchCV** | 873 ms ± 347| 740 ms ± 142 |


Unsurprisingly, we see that GridSearchCV and Ridge Regression from sklearn is the fastest in this context.
There is cost to distributing work and data, and as we previously mentioned, moving data from host to device.

### How does performance scale as we scale data?

```python
two_dup_data = np.array(np.vstack([X_train]*int(1e2)))
two_dup_train = np.array(np.hstack([y_train]*int(1e2)))
three_dup_data = np.array(np.vstack([X_train]*int(1e3)))
three_dup_train = np.array(np.hstack([y_train]*int(1e3)))

cu_grid = dcv.GridSearchCV(cu_clf, params, scoring='r2')
cu_grid.fit(two_dup_data, two_dup_train)

cu_grid = dcv.GridSearchCV(cu_clf, params, scoring='r2')
cu_grid.fit(three_dup_data, three_dup_train)

grid = dcv.GridSearchCV(clf, params, scoring='r2')
grid.fit(three_dup_data, three_dup_train)
```

Timing Measurements:

- cuml + dask-ml (2.8MB): 1min 17s
- cuml + dask-ml (28MB): 13.8 s
- sklearn + dask-ml: 4.87 s

cuML + dask-ml (distributed GridSearchCV) does significantly *worse* as data sizes increase!  Why?  Primarily, two reasons:

1. Non optimized movement of data between host and device compounded by N devices and the size of
the parameter space
2. Scoring methods are not implemented in with cuML

Below is the Dask graph for the GridSearch

<p>
  <a href="/images/cuml_grid.svg">
    <img src="/images/cuml_grid.svg" width="90%">
  </a>
</p>

There are 50 (cv=5 times 10 parameters for alpha) instances of chunking up our test data set and scoring performance.  That means 50 times we are moving data back forth between host and device for fitting and 50 times for scoring.  That's not great, but it's also very solvable -- build scoring functions for GPUs!

## Immediate Future Work

We know the problems, GH Issues have been filed, and we are working on these issues -- come help!

- [#242](https://github.com/rapidsai/cuml/issues/242)
- [#369](https://github.com/rapidsai/cuml/issues/369)
- [#2344](https://github.com/dask/distributed/issues/2344)

