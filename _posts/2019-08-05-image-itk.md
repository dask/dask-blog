---
layout: post
title: Load Large Image Data with Dask Array
author: John Kirkham, Matthew Rocklin, Matthew McCormick
tags: [imaging]
theme: twitter
---
{% include JB/setup %}

Executive Summary
-----------------

This post explores using the [ITK](TODO) suite of image processing utilities in parallel with Dask Array.
We cover ...

1.  A simple but common example of scaling deconvolution across a stack of 3d images
2.  Tips on how to make these two libraries work well together
2.  Challenges that we ran into and opportunities for future improvements.


A Worked Example
----------------

Let's start with a full example applying Richardson Lucy deconvolution to a
stack of light sheet microscopy data.  This is the same data that we showed how
to load in our last blogpost:

```python
# Load our data from last time¶
import dask.array as da
imgs = da.from_zarr("AOLLSMData_m4_raw.zarr/", "data")

# Load our Point Spread Function (PSF)
import dask.array.image
psf = dask.array.image.imread("AOLLSMData/m4/psfs_z0p1/*.tif")[:, None, ...]

# Convert data to float32 for computation¶
import numpy as np
imgs = imgs.astype(np.float32)
psf = psf.astype(np.float32)

# Apply Richardson-Lucy Deconvolution¶
def richardson_lucy_deconvolution(img, psf, iterations=1):
    """ Apply deconvolution to a single chunk of data """
    import itk

    img = img.squeeze()  # remove leading two length-one dimensions
    psf = psf.squeeze()  # remove leading two length-one dimensions

    image = itk.image_view_from_array(img)   # Convert to ITK object
    kernel = itk.image_view_from_array(psf)  # Convert to ITK object

    deconvolved = itk.richardson_lucy_deconvolution_image_filter(image,
                                                                 kernel_image=kernel,
                                                                 number_of_iterations=iterations)

    result = itk.array_from_image(deconvolved)  # Convert back to Numpy array
    result = result[None, None, ...]  # Add back the leading length-one dimensions

    return result

out = da.map_blocks(richardson_lucy_deconvolution, imgs, psf, dtype=np.float32)

# Trigger computation and store
out.to_zarr("AOLLSMData_m4_raw.zarr", "deconvolved", overwrite=True)
```

So in the example above we ...

1.  Load data both from Zarr and TIFF files into multi-chunked Dask arrays
2.  Construct a function to apply an ITK routine onto each chunk
3.  Apply that function across the dask array with the [map_blocks](TODO) method.
4.  Store the result back into Zarr format

TODO: motivate `map_blocks` a bit.

The challenging aspect (and where we
would expect most people to get stuck) is step 2, constructing a function to
apply an ITK routine into each chunk.  We dive into these challenges in the
next section:


Tips and common gotchas when using Dask and ITK together
--------------------------------------------------------

Ideally we would have been able to call `da.map_blocks` on the `itk.richardson_lucy_deconvolution_image_filter` function directly

```python
deconvolved = da.map_blocks(itk.richardson_lucy_deconvolution_image_filter, imgs, psf)
```

It would have been nice to avoid having to write the wrapper function that
unpacks and packs things, changes dimensions, and so on.  What were these steps
and why were they necessary?

TODO


Some future development opportunities
-------------------------------------

The example above should be enough for practitioners to use these tools
together today.  In this next section we discuss some enhancements that could
make these tools easier to use in the future:

Native NumPy handling in ITK
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TODO

Generalized Universal Functions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

TODO

GPU Acceleration
~~~~~~~~~~~~~~~~

TODO
