---
layout: post
title: Dask-ML and Ray have the same features for model selection
tagline: Modern hyperparameter optimizations, Scikit-learn support, framework support and scaling to many machines.
author: <a href="https://stsievert.com">Scott Sievert</a> (University of Wisconsin–Madison)
tags: [machine-learning, dask-ml, dask, ray]
theme: twitter
---

{% include JB/setup %}

Hyperparameter optimization is the process of deducing machine learning model
parameters that can't be learned from data. These "hyperparameters" have to be
given at model initialization. An example is with the `alpha` parameter in
Scikit-learn's [Ridge]. For a more complete overview, see "[How to Use t-SNE
Effectively][tsne]" or "[Tuning the hyper-parameters of an estimator][tuning]."

There's a host of libraries and frameworks out there to address this problem,
including a popular implementation [in Scikit-learn][skl-ms]. This interface
has been mirrored [in Dask-ML][dml-ms] and [auto-sklearn], both of which offer
advanced hyperparameter optimization features.  Dask-ML's implementation is
illustrated in "[Better and faster hyperparameter optimization with
Dask][db-bf]."

[auto-sklearn]:https://automl.github.io/auto-sklearn/master/

[Ray] recently provided a Scikit-learn interface to [Ray Tune].  [The
introduction][1] of this library, tune-sklearn, exaggerates it's novelty:

[Ray]:https://docs.ray.io
[Ray Tune]:https://docs.ray.io/en/master/tune.html

[dml-ms]:https://ml.dask.org/hyper-parameter-search.html
[skl-ms]:https://scikit-learn.org/stable/modules/grid_search.html
[tsne]:https://distill.pub/2016/misread-tsne/
[tuning]:https://scikit-learn.org/stable/modules/grid_search.html

[Ridge]:https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge
[1]:https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf

> Cutting edge hyperparameter tuning techniques (bayesian optimization, early
> stopping, distributed execution) can provide significant speedups over grid
> search and random search.
>
> However, the machine learning ecosystem is missing a solution that provides
> users with the ability to leverage these new algorithms while allowing users
> to stay within the Scikit-Learn API. In this blog post, we introduce
> tune-sklearn to bridge this gap. Tune-sklearn is a drop-in replacement for
> Scikit-Learn’s model selection module with state-of-the-art optimization
> features.
>
> —[GridSearchCV 2.0 — New and Improved][1]

This exaggerates Ray's novelty because Dask-ML and auto-sklearn also provide
"users with the ability to leverage these new algorithms while allowing users
to stay within the Scikit-Learn API." Ray is not the first project to "bridge
this gap."

That is, Dask-ML offers the same features that Ray's tune-sklearn offers:

[dml-hod]:https://ml.dask.org/hyper-parameter-search.html

> Here’s what tune-sklearn has to offer:
> 1. **Consistency with Scikit-Learn API** ...
> 2. **Modern hyperparameter tuning techniques** ...
> 3. **Framework support** ...
> 4. **Scale up** ...
>
> Tune-sklearn is also **fast**.

Dask-ML has had these features since June 2019, about a year earlier than Ray.
Let's look at each feature and see what Dask-ML and Ray have to offer:

> 1\. **Consistency with Scikit-Learn API**: tune-sklearn is a drop-in
> replacement for GridSearchCV and RandomizedSearchCV, so you only need to
> change less than 5 lines in a standard Scikit-Learn script to use the API.

Dask-ML is also consistent with the Scikit-learn API. Here's how to use
Dask-ML's hyperparameter optimization:

``` python
from dask_ml.model_selection import HyperbandSearchCV
from scipy.stats import loguniform
from sklearn.datasets import make_classification
from sklearn.linear_model import SGDClassifier

X, y = make_classification()
model = SGDClassifier()
params = {"loss": ["hinge", "log", "modified_huber", "squared_hinge", "perceptron"],
          "alpha": loguniform(1e-5, 1e-3)}

search = HyperbandSearchCV(model, params)
search.fit(X, y, classes=[0, 1])
```

This example is almost identical to [one of Ray's examples][sgd-eg] and [one of
Scikit-learn's examples][skl-eg]. Clearly, Dask-ML offers a Scikit-learn
compatible model selection API.

[skl-eg]:https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py
[sgd-eg]:https://github.com/ray-project/tune-sklearn/blob/31f228e21ef632a89a74947252d8ad5323cbd043/examples/sgd.py

> 2\. **Modern hyperparameter tuning techniques:** tune-sklearn is the only
> Scikit-Learn interface that allows you to easily leverage Bayesian
> Optimization, HyperBand, and other optimization techniques by simply toggling
> a few parameters.

Dask-ML also offers modern hyperparameter tuning techniques. Hyperband is the
most prominent example; after the introduction in 2016, [the paper by Li et.
al][hyperband-paper] has been cited [over 470 times][470] and has been
implemented in many different libraries including [Dask-ML][hscv],
[keras-tune], [Optuna], AutoML[^automl] and [Microsoft's NNI][nni].

Hyperband is so popular because it's so effective. In the introduction paper by
Li et.  al, they showed a rather drastic improvement when comparing with all
the relevant implementations,[^hyperband-figs] and this drastic improvement has
remained in follow-up works.[^follow-up]


[^follow-up]:See Figure 1 of [the BOHB paper][BOHB paper] and [a paper][blippar] from an augmented reality company.

[^hyperband-figs]:See Figures 4, 7 and 8 in "[Hyperband: A Novel Bandit-Based Approach toHyperparameter Optimization][hyperband-paper]."

[blippar]:https://arxiv.org/pdf/1801.01596.pdf
[BOHB paper]:http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf
[nni]:https://nni.readthedocs.io/en/latest/Tuner/HyperbandAdvisor.html

Even Bayesian hyperparameter optimization have been adapted to the Hyperband
implementation in AutoML's [BOHB]. Dask-ML could use more Bayesian sampling
approaches; however, this is difficult given the distributed nature of Dask and
the serial nature of Bayesian sampling algorithms. Bayesian sampling treats
models as black boxes, and tries to deduce the set of inputs that maximize the
black box's quality. If there were an infinite number of workers, the Bayesian
sampling algorithm wouldn't have time to deduce anything before initializing
models. Regardless, [BOHB] has an intelligent way of scaling to distributed
systems.[^scaling]

[^scaling]:As mentioned in Section 4.2 and illustrated in Figure 2 of [the BOHB paper][BOHB paper].

The additional features of Dask-ML's hyperparameter optimization are also
modern, especially in the context of natural language processing (NLP).
[Avoiding repeated work] in Scikit-learn Pipelines is a huge feature when
performing NLP because the initial stages of analysis take an excessive amount
of computation and memory.[^openai] It's future work to combine this with Dask-ML's
`HyperbandSearchCV`.

The Dask-ML model selection effectively avoids unnecessary work, both with
unpromising hyperparameters and with repeated work. Of course, as a machine
learning graduate student I see opportunities for future work, some of which
are mentioned in [the conclusion].

[the conclusion]:#conclusion
[hyperband-paper]:https://arxiv.org/pdf/1603.06560.pdf
[470]:https://scholar.google.com/scholar?cites=10473284631669296057&as_sdt=5,39&sciodt=0,39&hl=en

[^automl]:Their implementation of Hyperband in [HpBandSter] is included in [Auto-PyTorch] and [BOAH].

[^openai]:Computing [n-grams] is difficult. For OpenAI, NLP preprocessing took 8 GPU-months! ([source][gpt])

[n-grams]:https://en.wikipedia.org/wiki/N-gram
[gpt]:https://openai.com/blog/language-unsupervised/#drawbacks
[Avoiding repeated work]:https://ml.dask.org/hyper-parameter-search.html#avoid-repeated-work
[BOHB]:https://automl.github.io/HpBandSter/build/html/optimizers/bohb.html
[hscv]:https://ml.dask.org/modules/generated/dask_ml.model_selection.HyperbandSearchCV.html#dask_ml.model_selection.HyperbandSearchCV
[BOAH]:https://github.com/automl/BOAH
[Auto-PyTorch]:https://www.automl.org/wp-content/uploads/2018/09/chapter7-autonet.pdf
[HpBandSter]:https://github.com/automl/HpBandSter
[Optuna]:https://medium.com/optuna/optuna-supports-hyperband-93b0cae1a137
[keras-tune]:https://keras-team.github.io/keras-tuner/documentation/tuners/#hyperband-class

> 3\. **Framework support**: tune-sklearn is used primarily for tuning
> Scikit-Learn models, but it also supports and provides examples for many
> other frameworks with Scikit-Learn wrappers such as Skorch (Pytorch),
> KerasClassifiers (Keras), and XGBoostClassifiers (XGBoost).

Dask-ML model selection also supports libraries like Scikit-learn, PyTorch via
Skorch, KerasClassifier and XGBoost.

However, both Dask-ML and Ray have some qualifications. Certain libraries don't
offer an implementation of `partial_fit`,[^ray-pf] so not all of the modern
hyperparameter optimization techniques can be offered. That means Bayesian
sampling will have to be relied upon, which gets less and less relevant as the
number of workers grows. Here's a table comparing different libraries and their
support in Dask-ML and Ray:

| Library | Dask-ML support | Ray support | Dask-ML: early stopping? | Ray: early stopping? |
|:-----:|:-----:|:-----:|:-----:|:-----:|
| Scikit-learn | ✔ | ✔ | ✔, with `partial_fit` |✔, with `partial_fit` |
| PyTorch via Skorch | ✔ | ✔ | ✔ |✔ |
| Keras | ✔ |✔ | ❌|❌ |
| LightGBM | ✔ | ✔ | ❌ |❌ |
| XGBoost |✔ | ✔ | ❌ |❌ |

By this measure, Dask-ML and Ray model selection have the same level of
framework support. Of course, Dask has tangential integration with LightGBM and
XGBoost through [dask_ml.xgboost][dmlxg] and [dask-lightgbm][dml-lg].

[^ray-pf]:From [Ray's README.md]: "If the estimator does not support `partial_fit`, a warning will be shown saying early stopping cannot be done and it will simply run the cross-validation on Ray's parallel back-end."

[dml-lg]:https://github.com/dask/dask-lightgbm
[dmlxg]:https://ml.dask.org/xgboost.html
[Ray's README.md]:https://github.com/ray-project/tune-sklearn/blob/31f228e21ef632a89a74947252d8ad5323cbd043/README.md

> 4\. **Scale up**: Tune-sklearn leverages Ray Tune, a library for distributed
> hyperparameter tuning, to efficiently and transparently parallelize cross
> validation on multiple cores and even multiple machines.

Naturally, Dask-ML also scales to multiple cores/machines and
larger-than-memory datasets because it relies on Dask. Dask has wide support
for [different deployment options][ddo] that span from your personal machine to
supercomputers. Dask will very likely work on top of any computing system you
have available, including Kubernetes, SLURM and Hadoop clusters as well as your
personal machine.

In addition, I have benchmarked Dask-ML's model selection module how the
acceleration is affected by the number of Dask workers in "[Better and faster
hyperparameter optimization with Dask][db-bf]." That is, how does the
time-to-solution scale with the number of workers $P$? At first, it'll scale
like $1/P$ but with large number of workers the serial portion will dictate
time to solution according to [Amdahl's Law]. Briefly, for a fairly complicated
search I found Dask-ML's `HyperbandSearchCV` to saturate around 24 workers.

[^bohb-parallel]:In Section 4.2 of [their paper](http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf).

[db-bf]:https://blog.dask.org/2019/09/30/dask-hyperparam-opt

[Amdahl's Law]:https://en.wikipedia.org/wiki/Amdahl%27s_law
[ddo]:https://docs.dask.org/en/latest/setup.html
[plg]:https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html

> Tune-sklearn is also **fast**. To see this, we benchmark tune-sklearn (with
> early stopping enabled) against native Scikit-Learn and Dask GridSearchCV on
> a standard hyperparameter sweep.

Dask-ML is also fast according to "[Better and faster hyperparameter
optimization with Dask][db-bf]." However, that post only compares Sckit-learn
and Dask-ML. Now, there's a new implementation. Let's run a new benchmark, and
one that compares relevant implementations:

<!-- TODO: an experiment -->

## Conclusion

Dask-ML and Ray offer the same features for model selection: state-of-the-art
features with a Scikit-learn compatible API, and both implementations have
fairly wide support for different frameworks and rely on backends that can
scale to many machines.

The Ray implementation has provided motivation for further development,
specifically on the following items:

* Adding support for more libraries, including Keras. The issue tracking this
  is [dask/dask-ml#696][696].
* Including a Bayesian sampling implementation to Dask-ML's Hyperband
  implementation. The issue tracking this is [dask/dask-ml#697][697].
* Improving the implementation of initial/exploratory hyperparameter searches.
  An initial implementation is in [dask/dask-ml#532][532], which should be
  benchmarked against Ray.

[697]:https://github.com/dask/dask-ml/issues/697
[532]:https://github.com/dask/dask-ml/pull/532
[696]:https://github.com/dask/dask-ml/issues/696

---
