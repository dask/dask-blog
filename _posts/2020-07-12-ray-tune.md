---
layout: post
title: Dask-ML offers modern hyperparameter optimization techniques packaged in a Scikit-learn API
tagline: This has been implemented in Dask-ML for over a year
author: <a href="https://stsievert.com">Scott Sievert</a> (University of Wisconsin–Madison)
tags: [machine-learning, dask-ml, dask, ray]
theme: twitter
---

{% include JB/setup %}

Hyperparameter optimization is the process of deducing machine learning model
parameters that can't learned from data. These "hyperparameters" have to be
given at model initialization. An example is with the `alpha` parameter in
Scikit-learn's [Ridge]. For a more complete overview, see "[How to Use t-SNE
Effectively][tsne]" or "[Tuning the hyper-parameters of an estimator][tuning]."

[The introduction][1] of Ray's Scikit-learn compatible hyperparameter
optimization library made an counterfactual statement:

[tsne]:https://distill.pub/2016/misread-tsne/
[tuning]:https://scikit-learn.org/stable/modules/grid_search.html

[Ridge]:https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge
[1]:https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf

> Cutting edge hyperparameter tuning techniques (bayesian optimization, *early
> stopping, distributed execution*) can provide significant speedups over grid
> search and random search.
>
> However, the machine learning ecosystem is missing a solution that provides
> users with the ability to leverage these new algorithms while allowing users
> to stay within the Scikit-Learn API. In this blog post, we introduce
> tune-sklearn to bridge this gap. Tune-sklearn is a drop-in replacement for
> Scikit-Learn’s model selection module with state-of-the-art optimization
> features. [emphasis added]
>
> —[GridSearchCV 2.0 — New and Improved][1]

This is a patently false: Dask-ML fills the same gap they claim to bridge.
The Ray authors claim their library unique because it has these features:

[dml-hod]:https://ml.dask.org/hyper-parameter-search.html


> Here’s what tune-sklearn has to offer:
> 1. **Consistency with Scikit-Learn API** ...
> 2. **Modern hyperparameter tuning techniques** ...
> 3. **Framework support** ...
> 4. **Scale up** ...

Dask-ML has all of the same features. Dask-ML "bridged this gap" for over a
year.[^date] It's not clear to me how they missed the relevant implementations
in Dask-ML: all the implementations are in the same module, including the basic
module they test against (Dask-ML's `GridSearchCV`) and modern techniques like
Dask-ML's `HyperbandSearchCV`. These modern techniques are featured prominently
in the [Dask-ML's hyperparameter optimization documentation][dml-hod], which
clearly specifies the problems that may occur during hyperparameter
optimization.

Let's look at each feature and see what Dask-ML and Ray have to
offer:

[^date]:Dask-ML has had an implementation of `HyperbandSearchCV` since June 2019.

> 1\. **Consistency with Scikit-Learn API**: tune-sklearn is a drop-in
> replacement for GridSearchCV and RandomizedSearchCV, so you only need to
> change less than 5 lines in a standard Scikit-Learn script to use the API.

Dask-ML is also consistent with the Scikit-learn API. Here's how to use
Dask-ML's hyperparameter optimization:

``` python
from dask_ml.model_selection import HyperbandSearchCV
from scipy.stats import uniform, loguniform
from sklearn.datasets import make_classification
from sklearn.linear_model import SGDClassifier

X, y = make_classification()
model = SGDClassifier()
params = {"loss": ["hinge", "log", "modified_huber", "squared_hinge", "perceptron"],
          "alpha": loguniform(1e-5, 1e-3)}

search = HyperbandSearchCV(model, params)
search.fit(X, y, classes=[0, 1])
```

This usage is common for every member of Dask-ML's model selection module.
This example is almost identical to [one of Ray's examples][sgd-eg] and [one of
Scikit-learn's examples][skl-eg].

[skl-eg]:https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py
[sgd-eg]:https://github.com/ray-project/tune-sklearn/blob/31f228e21ef632a89a74947252d8ad5323cbd043/examples/sgd.py

> 2\. **Modern hyperparameter tuning techniques:** tune-sklearn is the only
> Scikit-Learn interface that allows you to easily leverage Bayesian
> Optimization, HyperBand, and other optimization techniques by simply toggling
> a few parameters.

Dask-ML also offers modern hyperparameter tuning techniques. Hyperband is the
most prominent example; after the introduction in 2016, [the paper by Li et.
al][hyperband-paper] has been cited [over 470 times][470] and has been
implemented in many different libraries including [Dask-ML][hscv],
[keras-tune], [Optuna], AutoML[^automl] and [Microsoft's NNI][nni].

It's so popular because it's so effective. In the introduction paper by Li et.
al, they showed a rather drastic improvement when comparing with all the
relevant implementations.[^hyperband-figs] This drastic improvement has
remained in follow-up works.[^follow-up]

[nni]:https://nni.readthedocs.io/en/latest/Tuner/HyperbandAdvisor.html

[^follow-up]:See Figure 1 of [the BOHB paper] and [a paper][blippar] from an augmented reality company.

[^hyperband-figs]:See Figures 4, 7 and 8 in "[Hyperband: A Novel Bandit-Based Approach toHyperparameter Optimization][hyperband-paper]."

[blippar]:https://arxiv.org/pdf/1801.01596.pdf
[the BOHB paper]:http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf

Even Bayesian hyperparameter optimization has been adapted to the Hyperband
implementation in AutoML's [BOHB]. Dask-ML could use more Bayesian sampling
approaches; however, this is difficult given the distributed nature of Dask and
the serial nature of Bayesian sampling algorithms. Bayesian sampling treats
models as black boxes, and tries to deduce the set of inputs that maximize the
black box's quality. If there were an infinite number of workers, the Bayesian
sampling algorithm wouldn't have time to deduce anything before initializing
models.

The additional features of Dask-ML's hyperparameter optimization are also
modern, especially in the context of natural language processing (NLP).
[Avoiding repeated work] in Scikit-learn Pipelines is a huge feature when
performing NLP because the initial stages of analysis take the
longest.[^openai] It's future work to combine this with Dask-ML's
`HyperbandSearchCV`.

In short, there's no doubt Dask-ML's model selection is modern. Some additional
features can be added but they are additions to the core feature, a Hyperband
implementation. This especially true since Dask-ML's model selection supports
the most basic level of Bayesian sampling, a user-specified estimate through
`scipy.stats`.

[hyperband-paper]:https://arxiv.org/pdf/1603.06560.pdf
[470]:https://scholar.google.com/scholar?cites=10473284631669296057&as_sdt=5,39&sciodt=0,39&hl=en

[^automl]:Their implementation of Hyperband in [HpBandSter] is included in [Auto-PyTorch] and [BOAH].

[^openai]:For OpenAI, NLP preprocessing took 8 GPU-months! ([source][gpt])

[gpt]:https://openai.com/blog/language-unsupervised/#drawbacks
[Avoiding repeated work]:https://ml.dask.org/hyper-parameter-search.html#avoid-repeated-work
[BOHB]:https://automl.github.io/HpBandSter/build/html/optimizers/bohb.html
[hscv]:https://ml.dask.org/modules/generated/dask_ml.model_selection.HyperbandSearchCV.html#dask_ml.model_selection.HyperbandSearchCV
[BOAH]:https://github.com/automl/BOAH
[Auto-PyTorch]:https://www.automl.org/wp-content/uploads/2018/09/chapter7-autonet.pdf
[HpBandSter]:https://github.com/automl/HpBandSter
[Optuna]:https://medium.com/optuna/optuna-supports-hyperband-93b0cae1a137
[keras-tune]:https://keras-team.github.io/keras-tuner/documentation/tuners/#hyperband-class

> 3\. **Framework support**: tune-sklearn is used primarily for tuning
> Scikit-Learn models, but it also supports and provides examples for many
> other frameworks with Scikit-Learn wrappers such as Skorch (Pytorch),
> KerasClassifiers (Keras), and XGBoostClassifiers (XGBoost).

This is true for Dask-ML too. However, both Dask-ML and Ray have some
qualifications. Certain libraries don't offer an implementation of
`partial_fit`,[^ray-pf] so not all of the modern hyperparameter optimization
techniques can be offered. That means Bayesian sampling will have to be relied
upon, which gets less and less relevant as the number of workers grows. Here's
a table comparing different libraries and their support in Dask-ML and Ray:

| Library | Dask-ML support | Ray support | Dask-ML: early stopping? | Ray: early stopping? |
|:-----:|:-----:|:-----:|:-----:|:-----:|
| Scikit-learn | ✔ | ✔ | ✔, with `partial_fit` |✔, with `partial_fit` |
| PyTorch via Skorch | ✔ | ✔ | ✔ |✔ |
| Keras | ✔ |✔ | ❌|❌ |
| LightGDM | ✔ | ✔ | ❌ |❌ |
| XGBoost |✔ | ✔ | ❌ |❌ |

Of course, Dask has further integration with LightGDM and XGBoost through
[dask_ml.xgboost][dmlxg] and [dask-lightgdm][dml-lg].

[^ray-pf]:From [Ray's README.md]: "If the estimator does not support `partial_fit`, a warning will be shown saying early stopping cannot be done and it will simply run the cross-validation on Ray's parallel back-end."

[dml-lg]:https://github.com/dask/dask-lightgbm
[dmlxg]:https://ml.dask.org/xgboost.html
[Ray's README.md]:https://github.com/ray-project/tune-sklearn/blob/31f228e21ef632a89a74947252d8ad5323cbd043/README.md

> 4\. **Scale up**: Tune-sklearn leverages Ray Tune, a library for distributed
> hyperparameter tuning, to efficiently and transparently parallelize cross
> validation on multiple cores and even multiple machines.

Naturally, this is true for Dask-ML too, especially with Dask's wide support of
[different deployment options][ddo] that span from your personal machine to
supercomputers. Dask will very likely work on top of any computing system you
have available.

In addition, I have run benchmarks to see where the acceleration starts to
saturate in "[Better and faster hyperparameter optimization with Dask][db-bf]."
That is, how does the time-to-solution scale with the number of workers $N$? At
first, it'll scale like $1/N$ but with large $N$ the serial portion will
dictate time to solution according to [Amdahl's Law]. Briefly, for a fairly
complicated search I found Dask-ML's `HyperbandSearchCV` to saturate around 24
workers.

[^bohb-parallel]:In Section 4.2 of [their paper](http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf).

[db-bf]:https://blog.dask.org/2019/09/30/dask-hyperparam-opt

[Amdahl's Law]:https://en.wikipedia.org/wiki/Amdahl%27s_law
[ddo]:https://docs.dask.org/en/latest/setup.html
[plg]:https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html

## Conclusion

Ray tune-sklearn does not "bridge the gap" between "leverag[ing] new algorithms
while allowing users to stay within the Scikit-Learn API." Dask-ML has offered
this for over a year, since June 2019.

That said, the Ray implementation has provided motivation for further
development, specifically on the following items:

* Adding support for more libraries. The issue tracking this is
  [dask/dask-ml#696][696].
* Including a Bayesian sampling implementation in Dask-ML's Hyperband
  implementation. The issue tracking this is [dask/dask-ml#697][697].
* Improving the implementation of initial/exploratory hyperparameter
  searches. An initial implementation is in [dask/dask-ml#532][532], which
  should be benchmarked against Ray.

[697]:https://github.com/dask/dask-ml/issues/697
[532]:https://github.com/dask/dask-ml/pull/532
[696]:https://github.com/dask/dask-ml/issues/696

---
