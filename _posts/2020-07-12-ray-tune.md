---
layout: post
title: Dask-ML and Ray have the same features for model selection
tagline: Modern hyperparameter optimizations, Scikit-learn support, framework support and scaling to many machines.
author: <a href="https://stsievert.com">Scott Sievert</a> (University of Wisconsin–Madison)
tags: [machine-learning, dask-ml, dask, ray]
theme: twitter
---

*This post also lives at http://stsievert.com/2020/03/ (with better styling)*

{% include JB/setup %}

Hyperparameter optimization is the process of deducing machine learning model
parameters that can't be learned from data. These "hyperparameters" have to be
given at model initialization. An example is with the `alpha` parameter in
Scikit-learn's [Ridge]. For a more complete overview, see "[How to Use t-SNE
Effectively][tsne]" or "[Tuning the hyper-parameters of an estimator][tuning]."

There's a host of libraries and frameworks out there to address this problem,
including a popular implementation [in Scikit-learn][skl-ms]. This interface
has been mirrored [in Dask-ML][dml-ms] and [auto-sklearn], both of which offer
advanced hyperparameter optimization features.  Dask-ML's implementation is
illustrated in "[Better and faster hyperparameter optimization with
Dask][db-bf]."

[auto-sklearn]:https://automl.github.io/auto-sklearn/master/

[Ray] recently provided a wrapper to [Ray Tune] that mirrors the Scikit-learn
API. [The introduction][1] of this library states the following:

[Ray]:https://docs.ray.io
[Ray Tune]:https://docs.ray.io/en/master/tune.html

[dml-ms]:https://ml.dask.org/hyper-parameter-search.html
[skl-ms]:https://scikit-learn.org/stable/modules/grid_search.html
[tsne]:https://distill.pub/2016/misread-tsne/
[tuning]:https://scikit-learn.org/stable/modules/grid_search.html

[Ridge]:https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge
[1]:https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf

> Cutting edge hyperparameter tuning techniques (bayesian optimization, early
> stopping, distributed execution) can provide significant speedups over grid
> search and random search.
>
> However, the machine learning ecosystem is missing a solution that provides
> users with the ability to leverage these new algorithms while allowing users
> to stay within the Scikit-Learn API. In this blog post, we introduce
> tune-sklearn [Ray's tuning library] to bridge this gap. Tune-sklearn is a
> drop-in replacement for Scikit-Learn’s model selection module with
> state-of-the-art optimization features.
>
> —[GridSearchCV 2.0 — New and Improved][1]

This claim is inaccurate: Dask-ML has provided access to "cutting edge
hyperparameter tuning techniques" with a Scikit-learn compatible API for over a
year. To provide facts to correct the inaccuracy, let's start by looking at the
features of Ray's tune-sklearn:

[dml-hod]:https://ml.dask.org/hyper-parameter-search.html

> Here’s what [Ray's] tune-sklearn has to offer:
> 1. **Consistency with Scikit-Learn API** ...
> 2. **Modern hyperparameter tuning techniques** ...
> 3. **Framework support** ...
> 4. **Scale up** ...
>
> [Ray's] Tune-sklearn is also **fast**.

Dask-ML's model selection module has every one of the features. Let's look at
each feature and see what Dask-ML and Ray have to offer:

> 1\. **Consistency with Scikit-Learn API**: tune-sklearn is a drop-in
> replacement for GridSearchCV and RandomizedSearchCV, so you only need to
> change less than 5 lines in a standard Scikit-Learn script to use the API.

Dask-ML is also consistent with the Scikit-learn API. Here's how to use
Dask-ML's hyperparameter optimization:

``` python
from dask_ml.model_selection import HyperbandSearchCV
from dask.distributed import Client
from scipy.stats import loguniform
from sklearn.datasets import make_classification
from sklearn.linear_model import SGDClassifier

if __name__ == "__main__":
    X, y = make_classification()
    model = SGDClassifier()
    params = {"loss": ["hinge", "log", "modified_huber", "squared_hinge", "perceptron"],
              "alpha": loguniform(1e-5, 1e-3)}

    client = Client()
    search = HyperbandSearchCV(model, params)
    search.fit(X, y, classes=[0, 1])
```

This example is almost identical to [one of Ray's examples][sgd-eg] and [one of
Scikit-learn's examples][skl-eg]. Clearly, Dask-ML offers a Scikit-learn
compatible model selection API.

[skl-eg]:https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py
[sgd-eg]:https://github.com/ray-project/tune-sklearn/blob/31f228e21ef632a89a74947252d8ad5323cbd043/examples/sgd.py

> 2\. **Modern hyperparameter tuning techniques:** tune-sklearn is the only
> Scikit-Learn interface that allows you to easily leverage Bayesian
> Optimization, HyperBand, and other optimization techniques by simply toggling
> a few parameters.

Dask-ML also offers state-of-the-art hyperparameter tuning techniques that
avoid unnecessary work. Hyperband is the most prominent implementation. After
the introduction of Hyperband in 2016 by Li et. al, [the
paper][hyperband-paper] has been cited [over 470 times][470] and has been
implemented in many different libraries including [Dask-ML][hscv],
[keras-tune], [Optuna], AutoML[^automl] and [Microsoft's NNI][nni].

Hyperband is popular because it avoids repeated work. At the most basic level,
Hyperband is an "early stopping" technique for Scikit-learn's
`RandomizedSearchCV`: it stops low performing at certain points in the training
process.[^stopping] This works *very* well: the original paper shows a rather
drastic improvement over all the relevant implementations,[^hyperband-figs] and
this drastic improvement persists in follow-up works.[^follow-up] Here's an
illustration of one of these results:

[^stopping]:Li et. al have some theory that determines how many models to stop when.

[^follow-up]:See Figure 1 of [the BOHB paper][BOHB paper] and [a paper][blippar] from an augmented reality company.

[^hyperband-figs]:See Figures 4, 7 and 8 in "[Hyperband: A Novel Bandit-Based Approach toHyperparameter Optimization][hyperband-paper]."

[blippar]:https://arxiv.org/pdf/1801.01596.pdf
[BOHB paper]:http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf
[nni]:https://nni.readthedocs.io/en/latest/Tuner/HyperbandAdvisor.html

Here's one of these results, pulled from Figures 7 and 8 of the [Hyperband
paper][hyperband-paper]:

<img width="80%" src="/images/2020-model-selection/hyperband-fig-7-8.png" />

Even Bayesian hyperparameter optimization has been adapted to the Hyperband
implementation in AutoML's [BOHB]. Dask-ML could use more Bayesian sampling
approaches; however, this is difficult in any distributed system with the
serial nature of Bayesian sampling algorithms. Bayesian sampling treats models
as black boxes, and tries to deduce the set of inputs that maximize the black
box's quality. This is difficult in a distributed system because if there were
an infinite number of workers, the Bayesian sampling algorithm wouldn't have
time to deduce anything before initializing models. Regardless, [BOHB] has an
intelligent way of scaling to distributed systems given Hyperband's parallel
architecture.[^scaling]

This also looks useful towards the end of the hyperparameter optimization
according to Figure 1 from the [BOHB paper]:

<img width="50%" src="/images/2020-model-selection/bohb-fig-1.png" />

[^scaling]:As mentioned in Section 4.2 and illustrated in Figure 2 of [the BOHB paper][BOHB paper].

There are additional useful features of Dask-ML's hyperparameter optimization,
especially in the context of natural language processing (NLP).  [Avoiding
repeated work] in Scikit-learn Pipelines is a huge feature when performing NLP
because the initial stages of analysis take an excessive amount of computation
and memory.[^openai] Currently, Dask-ML's implementation that avoid repeated
work is completely separate from Dask-ML's Hyperband implementation --
naturally it's future work to combine the two.

The Dask-ML model selection effectively avoids unnecessary work, both with
unpromising hyperparameters and with unnecessary repeated work. Of course, as a
machine learning graduate student I see opportunities for future work, some of
which are mentioned in [the conclusion]. Luckily, Dask-ML's model selection
framework is pretty flexible so these modifications are straightforward.

[the conclusion]:#conclusion
[hyperband-paper]:https://arxiv.org/pdf/1603.06560.pdf
[470]:https://scholar.google.com/scholar?cites=10473284631669296057&as_sdt=5,39&sciodt=0,39&hl=en

[^automl]:Their implementation of Hyperband in [HpBandSter] is included in [Auto-PyTorch] and [BOAH].

[^openai]:Computing [n-grams] is difficult. For OpenAI, NLP preprocessing took 8 GPU-months! ([source][gpt])

[n-grams]:https://en.wikipedia.org/wiki/N-gram
[gpt]:https://openai.com/blog/language-unsupervised/#drawbacks
[Avoiding repeated work]:https://ml.dask.org/hyper-parameter-search.html#avoid-repeated-work
[BOHB]:https://automl.github.io/HpBandSter/build/html/optimizers/bohb.html
[hscv]:https://ml.dask.org/modules/generated/dask_ml.model_selection.HyperbandSearchCV.html#dask_ml.model_selection.HyperbandSearchCV
[BOAH]:https://github.com/automl/BOAH
[Auto-PyTorch]:https://www.automl.org/wp-content/uploads/2018/09/chapter7-autonet.pdf
[HpBandSter]:https://github.com/automl/HpBandSter
[Optuna]:https://medium.com/optuna/optuna-supports-hyperband-93b0cae1a137
[keras-tune]:https://keras-team.github.io/keras-tuner/documentation/tuners/#hyperband-class

> 3\. **Framework support**: tune-sklearn is used primarily for tuning
> Scikit-Learn models, but it also supports and provides examples for many
> other frameworks with Scikit-Learn wrappers such as Skorch (Pytorch),
> KerasClassifiers (Keras), and XGBoostClassifiers (XGBoost).

Dask-ML model selection also supports libraries like Scikit-learn, PyTorch (via
Skorch), Keras (via KerasClassifier) and XGBoost.

However, both Dask-ML and Ray have some qualifications. Certain libraries don't
offer an implementation of `partial_fit`,[^ray-pf] so not all of the modern
hyperparameter optimization techniques can be offered. That means Bayesian
sampling will have to be relied upon, which gets less and less relevant as the
number of workers grows. Here's a table comparing different libraries and their
support in Dask-ML's model selection and Ray's tune-sklearn:

| Library | Dask-ML support | Ray support | Dask-ML: early stopping? | Ray: early stopping? |
|:-----:|:-----:|:-----:|:-----:|:-----:|
| Scikit-learn | ✔ | ✔ | ✔, with `partial_fit` |✔, with `partial_fit` |
| PyTorch via Skorch | ✔ | ✔ | ✔ |✔ |
| Keras | ✔ |✔ | ❌|❌ |
| LightGBM | ✔ | ✔ | ❌ |❌ |
| XGBoost |✔ | ✔ | ❌ |❌ |

By this measure, Dask-ML and Ray model selection have the same level of
framework support. Of course, Dask has tangential integration with LightGBM and
XGBoost through [dask_ml.xgboost][dmlxg] and [dask-lightgbm][dml-lg].

[^ray-pf]:From [Ray's README.md]: "If the estimator does not support `partial_fit`, a warning will be shown saying early stopping cannot be done and it will simply run the cross-validation on Ray's parallel back-end."

[dml-lg]:https://github.com/dask/dask-lightgbm
[dmlxg]:https://ml.dask.org/xgboost.html
[Ray's README.md]:https://github.com/ray-project/tune-sklearn/blob/31f228e21ef632a89a74947252d8ad5323cbd043/README.md

> 4\. **Scale up**: Tune-sklearn leverages Ray Tune, a library for distributed
> hyperparameter tuning, to efficiently and transparently parallelize cross
> validation on multiple cores and even multiple machines.

Naturally, Dask-ML also scales to multiple cores/machines because it relies on Dask. Dask has wide support
for [different deployment options][ddo] that span from your personal machine to
supercomputers. Dask will very likely work on top of any computing system you
have available, including Kubernetes, SLURM and Hadoop clusters as well as your
personal machine.

Dask-ML's model selection also scales to larger than memory datasets.

[ray-trainable]:https://medium.com/rapids-ai/30x-faster-hyperparameter-search-with-raytune-and-rapids-403013fbefc5

In addition, I have benchmarked Dask-ML's model selection module how the
acceleration is affected by the number of Dask workers in "[Better and faster
hyperparameter optimization with Dask][db-bf]." That is, how does the
time-to-solution scale with the number of workers $P$? At first, it'll scale
like $1/P$ but with large number of workers the serial portion will dictate
time to solution according to [Amdahl's Law]. Briefly, for a fairly complicated
search I found Dask-ML's `HyperbandSearchCV` to saturate around 24 workers.

[^bohb-parallel]:In Section 4.2 of [their paper](http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf).

[db-bf]:https://blog.dask.org/2019/09/30/dask-hyperparam-opt

[Amdahl's Law]:https://en.wikipedia.org/wiki/Amdahl%27s_law
[ddo]:https://docs.dask.org/en/latest/setup.html
[plg]:https://lightgbm.readthedocs.io/en/latest/Parallel-Learning-Guide.html

> Tune-sklearn is also **fast**. To see this, we benchmark tune-sklearn (with
> early stopping enabled) against native Scikit-Learn and Dask GridSearchCV on
> a standard hyperparameter sweep.

Dask-ML is also fast according to "[Better and faster hyperparameter
optimization with Dask][db-bf]." However, that post only compares Sckit-learn
and Dask-ML. Now, there's a new implementation. Let's run a new benchmark, and
one that compares relevant implementations:

<!-- TODO: an experiment -->

## Conclusion

Dask-ML and Ray offer the same features for model selection: state-of-the-art
features with a Scikit-learn compatible API, and both implementations have
fairly wide support for different frameworks and rely on backends that can
scale to many machines.

The Ray implementation has provided motivation for further development,
specifically on the following items:

1. **Including a Bayesian sampling implementation to Dask-ML's Hyperband
   implementation** that's similar to BOHB's ([dask/dask-ml#697][697]).
2. **Improving the implementation of exploratory hyperparameter searches.** An
   initial implementation is in [dask/dask-ml#532][532], which should be
   benchmarked against Ray.
3. **Adding support for more libraries, including Keras**
   ([dask/dask-ml#696][696]).  We have already adding a `partial_fit`
   implementation to a Scikit-learn wrapper for Keras, SciKeras
   ([scikeras#17]), and are following [tensorflow#39609].

The Dask-ML model selection framework is pretty flexible. All of these pieces
of development are straightforward modifications.

[scikit-learn#3299]:https://github.com/scikit-learn/scikit-learn/issues/3299
[tensorflow#39609]:https://github.com/tensorflow/tensorflow/pull/39609
[scikeras#17]:https://github.com/adriangb/scikeras/pull/17

[697]:https://github.com/dask/dask-ml/issues/697
[532]:https://github.com/dask/dask-ml/pull/532
[696]:https://github.com/dask/dask-ml/issues/696

---
